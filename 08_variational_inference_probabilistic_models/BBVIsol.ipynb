{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gaussian Model with BBVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate data from a simple model: Normal(10, 1)\n",
    "data = np.random.normal(loc = 10, scale = 1, size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual estimation of the gradient of the ELBO for the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient estimator using sampling -- vanilla BBVI\n",
    "# We here assume the model X ~ Normal(mu, 1)\n",
    "# with unknown mu, that in itself is Normal, mean 0 and standard deviation 1000, \n",
    "# so effectively an uniformed prior. \n",
    "# The variational dstribution for mu is also Normal, with parameter q_mu_lambda\n",
    "# -- taking the role of lambda in the calculations -- and variance 1.\n",
    "\n",
    "def grad_estimate(q_mu_lambda, samples = 1):\n",
    "    # sum_grad_estimate will hold the sum as we move along over the <samples> samples. \n",
    "    sum_grad_estimate = 0\n",
    "    for i in range(samples):\n",
    "        # Sample one example from current best guess for the variational distribution\n",
    "        mu_sample = np.random.normal(loc=q_mu_lambda, scale=1, size=1)\n",
    "        \n",
    "        # Now we want to calculate the contribution from this sample, namely \n",
    "        # [log p(x, mu_sample) - log q(mu|lambda) ] * grad( log q(mu_sample|lambda) )\n",
    "        #\n",
    "        # First log p(x|mu_sample) + log p(mu_sample) - log q(mu_sample|lambda) \n",
    "        value = np.sum(norm.logpdf(data, loc=mu_sample, scale=1)) \n",
    "        + norm.logpdf(mu_sample, loc = 0, scale = 1000)  \n",
    "        - norm.logpdf(mu_sample, loc= q_mu_lambda, scale = 1)\n",
    "        \n",
    "        # Next grad (log q(mu_sample|lambda))\n",
    "        # The Normal distribution gives the score function with known variance as <value> - <mean>\n",
    "        grad_q = mu_sample - q_mu_lambda\n",
    "        \n",
    "        # grad ELBO for this sample is therefore in total given by\n",
    "        sum_grad_estimate = sum_grad_estimate + grad_q * value\n",
    "        \n",
    "    # Divide by number of samples to get average value -- the estimated expectation  \n",
    "    return sum_grad_estimate/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: [-11.60604789]\n",
      "Iteration 10: [-5.33705271]\n",
      "Iteration 20: [-5.58847896]\n",
      "Iteration 30: [-3.01285956]\n",
      "Iteration 40: [0.64420794]\n",
      "Iteration 50: [-2.25654332]\n",
      "Iteration 60: [0.59912873]\n",
      "Iteration 70: [4.42624524]\n",
      "Iteration 80: [4.00348581]\n",
      "Iteration 90: [5.2545006]\n",
      "Iteration 100: [5.77074475]\n",
      "Iteration 110: [6.3213011]\n",
      "Iteration 120: [6.57219151]\n",
      "Iteration 130: [6.69834762]\n",
      "Iteration 140: [7.27265731]\n",
      "Iteration 150: [7.43427746]\n",
      "Iteration 160: [7.73134501]\n",
      "Iteration 170: [8.10168]\n",
      "Iteration 180: [8.24512834]\n",
      "Iteration 190: [8.30006127]\n",
      "Iteration 200: [8.18304935]\n",
      "Iteration 210: [8.42605926]\n",
      "Iteration 220: [8.56945759]\n",
      "Iteration 230: [8.9500815]\n",
      "Iteration 240: [8.85885982]\n",
      "Iteration 250: [8.95452735]\n",
      "Iteration 260: [9.03092096]\n",
      "Iteration 270: [9.10848151]\n",
      "Iteration 280: [9.06763766]\n",
      "Iteration 290: [9.25002527]\n",
      "Iteration 300: [9.28287303]\n",
      "Iteration 310: [9.28340427]\n",
      "Iteration 320: [9.48627303]\n",
      "Iteration 330: [9.74189249]\n",
      "Iteration 340: [9.7073375]\n",
      "Iteration 350: [9.67765973]\n",
      "Iteration 360: [9.63169689]\n",
      "Iteration 370: [9.75438652]\n",
      "Iteration 380: [9.76490705]\n",
      "Iteration 390: [9.70908009]\n",
      "Iteration 400: [9.85356788]\n",
      "Iteration 410: [9.91742773]\n",
      "Iteration 420: [9.99862188]\n",
      "Iteration 430: [10.04743061]\n",
      "Iteration 440: [10.07753178]\n",
      "Iteration 450: [9.99352822]\n",
      "Iteration 460: [10.05536123]\n",
      "Iteration 470: [10.18590483]\n",
      "Iteration 480: [10.14450796]\n",
      "Iteration 490: [10.05371429]\n",
      "Iteration 500: [9.98474934]\n",
      "Iteration 510: [10.10751977]\n",
      "Iteration 520: [10.13674622]\n",
      "Iteration 530: [10.11547067]\n",
      "Iteration 540: [10.05459469]\n",
      "Iteration 550: [10.06027903]\n",
      "Iteration 560: [10.10319553]\n",
      "Iteration 570: [10.08115022]\n",
      "Iteration 580: [9.98061837]\n",
      "Iteration 590: [9.89242241]\n",
      "Iteration 600: [9.98814915]\n",
      "Iteration 610: [10.06815772]\n",
      "Iteration 620: [10.07040347]\n",
      "Iteration 630: [10.17044765]\n",
      "Iteration 640: [10.18245786]\n",
      "Iteration 650: [10.24005074]\n",
      "Iteration 660: [10.1686833]\n",
      "Iteration 670: [9.99831452]\n",
      "Iteration 680: [9.97286467]\n",
      "Iteration 690: [9.90447678]\n",
      "Iteration 700: [10.15129426]\n",
      "Iteration 710: [10.18816117]\n",
      "Iteration 720: [10.05319421]\n",
      "Iteration 730: [9.96554327]\n",
      "Iteration 740: [9.93418531]\n",
      "Iteration 750: [9.82084824]\n",
      "Iteration 760: [9.95838547]\n",
      "Iteration 770: [10.13076626]\n",
      "Iteration 780: [10.10904687]\n",
      "Iteration 790: [10.07527312]\n",
      "Iteration 800: [9.98246911]\n",
      "Iteration 810: [10.00012951]\n",
      "Iteration 820: [9.84661179]\n",
      "Iteration 830: [9.9854027]\n",
      "Iteration 840: [10.03646898]\n",
      "Iteration 850: [10.10841107]\n",
      "Iteration 860: [10.14432224]\n",
      "Iteration 870: [10.18993602]\n",
      "Iteration 880: [10.08183673]\n",
      "Iteration 890: [10.28663609]\n",
      "Iteration 900: [10.30861903]\n",
      "Iteration 910: [10.19356924]\n",
      "Iteration 920: [10.32997352]\n",
      "Iteration 930: [10.10181789]\n",
      "Iteration 940: [9.87659733]\n",
      "Iteration 950: [9.98439783]\n",
      "Iteration 960: [9.95633462]\n",
      "Iteration 970: [9.95756324]\n",
      "Iteration 980: [9.91659343]\n",
      "Iteration 990: [9.74577776]\n"
     ]
    }
   ],
   "source": [
    "q_mu_lambda = -10\n",
    "\n",
    "for i in range(1000):\n",
    "    grad = grad_estimate(q_mu_lambda, samples = 1)\n",
    "    q_mu_lambda = q_mu_lambda + 0.0001 * grad\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}: {q_mu_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check effect of sample count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "no_loops = 500\n",
    "for sample_count in [1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 40, 50]:\n",
    "    ##### Starting point\n",
    "    q_mu = -10\n",
    "    start = time.time()\n",
    "\n",
    "    #loop a couple of times\n",
    "    for t in range(no_loops):\n",
    "        q_grad = grad_estimate(q_mu, samples=sample_count)\n",
    "        # Adjust learning rate according to the formula <start>/((1 + <t>/100)**1.5)\n",
    "        lr = 1E-4#*np.power((t//500. + 1), -1.5) \n",
    "        q_mu = q_mu + lr * q_grad\n",
    "\n",
    "    print(\"{:4d} sample(s) -- Estimate: {:9.5f}; error {:5.1f}%  --  Calc.time: {:5.2f} sec.\".format(\n",
    "        sample_count, float(q_mu), float(10*np.abs(q_mu-10)), time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the variation in gradient estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the variation / \"unreliability\" of the gradient estimate we repeat \n",
    "# several times for the same lambda value and notice difference\n",
    "\n",
    "# Location to check -- close to the data mean (at +10). \n",
    "# The prior will move the variational optimium **slightly** away from the data mean, \n",
    "# but due to the large prior variance of mu this should be a very limited effect.\n",
    "# We should therefore expect a positive derivative (since we want to move \n",
    "# q_mu_lambda towards the data mean, that is, **increase** it)\n",
    "q_mu_lambda = 9\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.set()\n",
    "# Do with different sample sizes\n",
    "for sample_count in [1, 2, 3, 4, 5, 10, 25]:\n",
    "\n",
    "    #loop\n",
    "    q_grad = []\n",
    "    for t in range(500):\n",
    "        q_grad.append(grad_estimate(q_mu_lambda, samples=sample_count))\n",
    "    \n",
    "    sns.distplot(q_grad, hist=False, label=\"$M = {:d}$\".format(sample_count))\n",
    "    \n",
    "    # Report back\n",
    "    print(\"M = {:2d} sample(s) in BBVI -- Mean of gradient: {:7.3f}; Std.dev. of gradient: {:7.3f}\".format(\n",
    "        sample_count, np.mean(q_grad), np.std(q_grad)))\n",
    "\n",
    "plt.xlim([-500, 500])\n",
    "plt.show()      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "        lr = np.power((t/500. + 1), -1.5) \n",
    "        print(lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
