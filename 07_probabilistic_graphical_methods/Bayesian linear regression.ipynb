{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Uniform, Delta, Gamma, Binomial\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.optim as optim\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-detection",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "\n",
    "The following example is taken from \\[1\\].  We would like to explore the relationship between topographic heterogeneity of a nation as measured by the Terrain Ruggedness Index (variable *rugged* in the dataset) and its GDP per capita. In particular, it was noted by the authors in \\[1\\] that terrain ruggedness or bad geography is related to poorer economic performance outside of Africa, but rugged terrains have had a reverse effect on income for African nations. Let us look at the data \\[2\\] and investigate this relationship.  We will be focusing on three features from the dataset:\n",
    "  - `rugged`: quantifies the Terrain Ruggedness Index\n",
    "  - `cont_africa`: whether the given nation is in Africa\n",
    "  - `rgdppc_2000`: Real GDP per capita for the year 2000\n",
    " \n",
    "  \n",
    "We will take the logarithm for the response variable GDP as it tends to vary exponentially. We also use a new variable `african_rugged`, defined as the product between the variables `rugged` and `cont_africa`, to capture the correlation between ruggedness and whether a country is in Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
    "df[\"african_rugged\"] = data[\"cont_africa\"] * data[\"rugged\"]\n",
    "df = df[[\"cont_africa\", \"rugged\", \"african_rugged\", \"rgdppc_2000\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-seattle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display first 10 entries \n",
    "display(df[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "african_nations = data[data[\"cont_africa\"] == 1]\n",
    "non_african_nations = data[data[\"cont_africa\"] == 0]\n",
    "sns.scatterplot(non_african_nations[\"rugged\"], \n",
    "            np.log(non_african_nations[\"rgdppc_2000\"]), \n",
    "            ax=ax[0])\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "sns.scatterplot(african_nations[\"rugged\"], \n",
    "            np.log(african_nations[\"rgdppc_2000\"]), \n",
    "            ax=ax[1])\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-perfume",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning. Suppose we're given a dataset $\\mathcal{D}$ of the form\n",
    "\n",
    "$$ \\mathcal{D}  = \\{ (X_i, y_i) \\} \\qquad \\text{for}\\qquad i=1,2,...,N$$\n",
    "\n",
    "The goal of linear regression is to fit a function to the data of the form:\n",
    "\n",
    "$$ y = w X + b + \\epsilon $$\n",
    "\n",
    "where $w$ and $b$ are learnable parameters and $\\epsilon$ represents observation noise. Specifically $w$ is a matrix of weights and $b$ is a bias vector.\n",
    "\n",
    "Let's first implement linear regression in PyTorch and learn point estimates for the parameters $w$ and $b$.  Then we'll see how to incorporate uncertainty into our estimates by using Pyro to implement Bayesian regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-cooking",
   "metadata": {},
   "source": [
    "## 1.2 Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel():\n",
    "    def __init__(self):\n",
    "        self.w = torch.nn.Parameter(torch.zeros(1, 3))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def params(self):\n",
    "        return {\"b\":self.b, \"w\": self.w}\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        return (self.b + torch.mm(self.w, torch.t(x_data))).squeeze(0)\n",
    "\n",
    "regression_model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-favorite",
   "metadata": {},
   "source": [
    "## 1.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(regression_model.params().values(), lr=0.05)\n",
    "num_iterations = 5000\n",
    "data = torch.tensor(df.values, dtype=torch.float)\n",
    "x_data, y_data = data[:, :-1], data[:, -1]\n",
    "\n",
    "def main():\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model.predict(x_data)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 500 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.params().items():\n",
    "        print(name, param.data.numpy())\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-portal",
   "metadata": {},
   "source": [
    "## 1.2 Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Regression line \", fontsize=16)\n",
    "ax[0].scatter(x_data[x_data[:,0]==0,1].detach().numpy(), y_data[x_data[:,0]==0].detach().numpy())\n",
    "ax[1].scatter(x_data[x_data[:,0]==1,1].detach().numpy(), y_data[x_data[:,0]==1].detach().numpy())\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regression_model.predict(x_data[x_data[:,0]==0,:]).detach().numpy(), color='r')\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regression_model.predict(x_data[x_data[:,0]==1,:]).detach().numpy(), color='r')\n",
    "\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"Non African Nations\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"African Nations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-poster",
   "metadata": {},
   "source": [
    "# 2 Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-thought",
   "metadata": {},
   "source": [
    "# 2.1 Model specification\n",
    "\n",
    "<img src=\"LR-plate.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    # weight and bias priors\n",
    "    with pyro.plate(\"plate_w\", 3):\n",
    "        w = pyro.sample(\"w\", Normal(torch.zeros(1,1), torch.ones(1,1)))\n",
    "   \n",
    "    b = pyro.sample(\"b\", Normal(0., 1000.))\n",
    "    \n",
    "    with pyro.plate(\"plate_x\", len(x_data)):\n",
    "        # Compute the predicion mean\n",
    "        prediction_mean = (b + torch.mm(x_data,torch.t(w))).squeeze(-1)\n",
    "        # Introduce a Normal distribution and condition on the observed data\n",
    "        pyro.sample(\"y\", Normal(loc = prediction_mean, scale = torch.ones(1,1)), obs=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-houston",
   "metadata": {},
   "source": [
    "## 2.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "optim = Adam({\"lr\": 0.1})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO(), num_samples=10)\n",
    "\n",
    "num_iterations = 10000\n",
    "def train(x_data, y_data):\n",
    "    pyro.clear_param_store()\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(x_data, y_data)\n",
    "        if j % 500 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))\n",
    "\n",
    "train(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-pencil",
   "metadata": {},
   "source": [
    "## 2.3 Model uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionLineSample(x_data,guide):\n",
    "    return (guide()['b'] + torch.mm(x_data,torch.t(guide()['w']))).squeeze(-1)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Uncertainty in Regression line \", fontsize=16)\n",
    "ax[0].scatter(x_data[x_data[:,0]==0,1].detach().numpy(), y_data[x_data[:,0]==0].detach().numpy())\n",
    "ax[1].scatter(x_data[x_data[:,0]==1,1].detach().numpy(), y_data[x_data[:,0]==1].detach().numpy())\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regressionLineSample(x_data[x_data[:,0]==0,:],guide).detach().numpy(), color='r')\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regressionLineSample(x_data[x_data[:,0]==1,:],guide).detach().numpy(), color='r')\n",
    "\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"Non African Nations\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"African Nations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-beach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
